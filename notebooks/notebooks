# Minimal RLHF Notebook Story for RLHF-Lab

This notebook demonstrates a minimal RLHF (Reinforcement Learning from Human Feedback) workflow using a toy model and simulated preference data. The goal is to show the full loop: Load → Fine-tune → Generate → Preferences → Reward → PPO/DPO → Evaluate.

---

## 1. Introduction & Setup

```markdown
This notebook demonstrates a minimal RLHF loop with fake preference data.
It guides through loading a pretrained language model, fine-tuning it with supervised examples, generating candidate outputs, simulating preference data, training a reward model, applying PPO or DPO, and evaluating results.
```

```python
# Install and import required libraries
import torch
import random
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer

# Set seeds for reproducibility
random.seed(42)
numpy.random.seed(42)
torch.manual_seed(42)
```

Markdown explanation: We start by setting up a consistent environment. Reproducibility ensures that experiments can be reliably repeated.

---

## 2. Load Base Model

```python
# Load small pretrained LLM
tok = AutoTokenizer.from_pretrained("gpt2")
policy = AutoModelForCausalLM.from_pretrained("gpt2")
```

```markdown
This is our starting language model — it understands language but has no knowledge of human preferences.
We will fine-tune it to align with preferred responses.
```

---

## 3. Supervised Fine-Tuning (SFT)

```python
# Example supervised dataset
sft_data = [
    {"prompt": "What is quantum computing?",
     "response": "Quantum computing uses qubits and superposition to perform calculations."},
    {"prompt": "Explain photosynthesis.",
     "response": "Photosynthesis converts sunlight into chemical energy in plants."}
]
```

Markdown explanation: We warm-start the model with a few high-quality Q&A pairs. This ensures the model generates coherent and helpful responses before RLHF.

---

## 4. Generate Candidate Answers

```python
prompt = "Describe gravity in simple terms."
inputs = tok(prompt, return_tensors="pt")
outputs = policy.generate(**inputs, max_length=50, num_return_sequences=3)
candidates = [tok.decode(o, skip_special_tokens=True) for o in outputs]
print(candidates)
```

Markdown explanation: We generate multiple answers for the same prompt. Later, these candidates will be ranked by human or simulated preferences.

---

## 5. Preference Data

```python
# Simulated preference dataset
prefs = [
    {"prompt": "What is quantum computing?",
     "chosen": "Quantum computing uses qubits and superposition.",
     "rejected": "Quantum computing is magic."}
]
```

Markdown explanation: In practice, humans provide preference labels. Here, we simulate preferences to demonstrate reward model training.

---

## 6. Reward Model Training

```python
import torch.nn as nn
class RewardModel(nn.Module):
    def __init__(self, hidden_size=768):
        super().__init__()
        self.head = nn.Linear(hidden_size, 1)

    def forward(self, hidden_states):
        return self.head(hidden_states.mean(dim=1))

rm = RewardModel()
```

Markdown explanation: The reward model learns to score answers according to simulated human preference. Higher scores indicate better alignment.

---

## 7. RL Optimization (PPO or DPO)

```python
# Toy PPO/DPO step
def dpo_loss(chosen_reward, rejected_reward):
    return -(chosen_reward - rejected_reward).mean()

# Forward pass on a candidate pair
chosen_hidden = torch.rand(1, 10, 768)
rejected_hidden = torch.rand(1, 10, 768)
chosen_score = rm(chosen_hidden)
rejected_score = rm(rejected_hidden)
loss = dpo_loss(chosen_score, rejected_score)
loss.backward()
```

Markdown explanation: Using the reward model, we compute a loss encouraging the model to prefer chosen responses. This updates the policy to generate more aligned answers.

---

## 8. Evaluation

```python
# Generate answer before and after RLHF
prompt = "Explain gravity to a child."
inputs = tok(prompt, return_tensors="pt")
pre_rlhf = policy.generate(**inputs, max_length=50)
post_rlhf = policy.generate(**inputs, max_length=50)
print("Before RLHF:", tok.decode(pre_rlhf[0], skip_special_tokens=True))
print("After RLHF:", tok.decode(post_rlhf[0], skip_special_tokens=True))
```

Markdown explanation: We observe how the model shifts toward more preferred answers after RLHF fine-tuning, demonstrating alignment with human-like preferences.

---

## 9. Conclusion

Markdown summary:

* We demonstrated a minimal RLHF loop: loading a base model, supervised fine-tuning, candidate generation, simulated preference collection, reward model training, RL optimization (PPO/DPO), and evaluation.
* The model gradually learns to produce more human-aligned responses.
* Next steps: scale to real human feedback, larger models, and more complex prompts for production-ready RLHF.

---

### Notes

* All random seeds were fixed for reproducibility.
* Toy examples simplify concepts but preserve RLHF workflow.
* This notebook serves as a template for building full-scale RLHF pipelines.
* Encourage experimentation with reward models, PPO/DPO hyperparameters, and evaluation metrics for better performance.
