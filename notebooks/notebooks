# üìì RLHF Notebook Story ‚Äî Interactive & Styled Version for GitHub

This notebook demonstrates a minimal RLHF loop using GPT-2 and simulated preference data. To improve GitHub UI/UX, we include enhanced markdown styling, tables, callouts, and collapsible sections for readability.

---

## 1. Introduction & Setup

> **Goal:** Teach a small LLM to generate human-preferred responses using a stepwise RLHF approach.

```python
import torch
import random
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer

# Seeds for reproducibility
random.seed(42)
numpy.random.seed(42)
torch.manual_seed(42)
```

**Notes:**

* Ensures consistent results.
* Libraries: `torch`, `transformers`, `numpy`, `random`.

<details>
<summary>üí° Tip: Why seeds matter</summary>
Random seeds make your experiments reproducible, so outputs are deterministic across runs.
</details>

---

## 2. Load Base Model

```python
tok = AutoTokenizer.from_pretrained("gpt2")
policy = AutoModelForCausalLM.from_pretrained("gpt2")
```

* Base LLM is the ‚Äústudent.‚Äù
* Consider larger models (LLaMA, Falcon) for serious experiments.

<details>
<summary>üí° Tip: Tokenizers</summary>
Always use the matching tokenizer and model for consistency.
</details>

---

## 3. Supervised Fine-Tuning (SFT)

```python
sft_dataset = [{"prompt": "What is AI?", "response": "AI simulates human intelligence through algorithms."}]
# Define DataLoader, optimizer, and train loop
```

* SFT biases model toward safe, helpful responses.
* Use large datasets for better performance.

---

## 4. Generate Candidate Answers

```python
prompt = "Explain quantum computing simply."
inputs = tok(prompt, return_tensors="pt")
outputs = [policy.generate(**inputs, max_length=50) for _ in range(3)]
candidate_answers = [tok.decode(out[0], skip_special_tokens=True) for out in outputs]
print(candidate_answers)
```

* Generate multiple responses to compare preferences.
* Use stochastic sampling for diversity.

<details>
<summary>üí° Tip: Sampling Strategies</summary>
Temperature, top-k, and top-p sampling produce varied outputs instead of repetitive answers.
</details>

---

## 5. Preference Data

```python
prefs = [
    {"prompt": "What is quantum computing?",
     "chosen": "Quantum computing uses qubits and superposition.",
     "rejected": "Quantum computing is magic."}
]
```

* Foundation for reward model training.
* Ensure diversity across domains.

---

## 6. Reward Model Training

```python
import torch.nn as nn

class RewardModel(nn.Module):
    def __init__(self, hidden_size=768):
        super().__init__()
        self.head = nn.Linear(hidden_size, 1)
    def forward(self, hidden_states):
        return self.head(hidden_states.mean(dim=1))

# Train on preference pairs using pairwise loss
```

* Reduces manual labeling workload.
* Validate often to prevent misalignment.

---

## 7. RL Optimization (PPO/DPO)

```python
for batch in generated_batches:
    rewards = reward_model(batch)
    # PPO: compute advantage, clipped surrogate objective
    # DPO: maximize likelihood of preferred responses
```

* Aligns model output with human preferences.
* Start small, then scale.
* Track entropy to maintain diversity.

---

## 8. Evaluation

```python
pre_train_output = policy.generate(**inputs, max_length=50)
post_train_output = policy.generate(**inputs, max_length=50)
print("Before:", tok.decode(pre_train_output[0]))
print("After:", tok.decode(post_train_output[0]))
```

* Metrics: reward model scores, BLEU/ROUGE, semantic similarity.
* Combine automatic and human evaluation.

---

## 9. Conclusion & Next Steps

**Achievements:**

| Step | Description                |
| ---- | -------------------------- |
| 1    | Loaded base LLM            |
| 2    | Fine-tuned SFT             |
| 3    | Generated candidates       |
| 4    | Created preference pairs   |
| 5    | Trained reward model       |
| 6    | Optimized policy (PPO/DPO) |
| 7    | Evaluated improvements     |

**Next Steps:**

* Scale to larger datasets & real human feedback.
* Upgrade reward model architectures.
* Monitor safety, fairness, and alignment.
* Deploy efficiently on GPUs/TPUs.

**Analogy:**
Teaching a student:

1. Show examples (SFT).
2. Generate work (candidates).
3. Score like a teacher (reward).
4. Guide improvement (PPO/DPO).
5. Repeat until mastery (evaluation).

<details>
<summary>üí° Key Insight</summary>
RLHF is iterative, preference-based learning transforming a generic LLM into a human-aligned AI.
</details>
