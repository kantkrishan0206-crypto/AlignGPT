<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>RLHF Notebook Story</title>
<style>
    body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; background: #f9f9f9; color: #333; }
    h1, h2, h3 { color: #1f3b6c; }
    pre { background: #2d2d2d; color: #f8f8f2; padding: 10px; overflow-x: auto; border-radius: 5px; }
    code { font-family: monospace; }
    .tip { background: #e7f3ff; border-left: 4px solid #1f3b6c; padding: 10px; margin: 10px 0; }
    .section { background: #fff; padding: 15px; border-radius: 8px; margin-bottom: 20px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
    details { margin: 10px 0; padding: 10px; background: #f0f0f0; border-radius: 5px; }
    summary { font-weight: bold; cursor: pointer; }
    table { border-collapse: collapse; width: 100%; margin-top: 10px; }
    th, td { border: 1px solid #ccc; padding: 8px; text-align: left; }
    th { background: #1f3b6c; color: white; }
</style>
</head>
<body>

<h1>ðŸ““ RLHF Notebook Story â€” Interactive Guide</h1>
<p>This interactive HTML version narrates a minimal RLHF loop using GPT-2 and simulated preference data. Learn step-by-step from loading a base model to PPO/DPO optimization with explanations, code, and tips.</p>

<div class="section">
<h2>1. Introduction & Setup</h2>
<p><strong>Goal:</strong> Teach a small LLM to generate human-preferred responses stepwise.</p>
<pre><code>import torch
import random
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer

random.seed(42)
np.random.seed(42)
torch.manual_seed(42)</code></pre>
<div class="tip">ðŸ’¡ Tip: Seeds make your experiments reproducible.</div>
</div>

<div class="section">
<h2>2. Load Base Model</h2>
<pre><code>tok = AutoTokenizer.from_pretrained("gpt2")
policy = AutoModelForCausalLM.from_pretrained("gpt2")</code></pre>
<div class="tip">ðŸ’¡ Tip: Base model is your student. Larger models generalize better.</div>
</div>

<div class="section">
<h2>3. Supervised Fine-Tuning (SFT)</h2>
<p>Warm-start the model with high-quality Q&A pairs.</p>
<pre><code>sft_dataset = [{"prompt": "What is AI?", "response": "AI simulates human intelligence through algorithms."}]
# Implement DataLoader, optimizer, and train loop</code></pre>
</div>

<div class="section">
<h2>4. Generate Candidate Answers</h2>
<pre><code>prompt = "Explain quantum computing simply."
inputs = tok(prompt, return_tensors="pt")
outputs = [policy.generate(**inputs, max_length=50) for _ in range(3)]
candidate_answers = [tok.decode(out[0], skip_special_tokens=True) for out in outputs]
print(candidate_answers)</code></pre>
<details>
<summary>ðŸ’¡ Tip: Sampling Strategies</summary>
<p>Use temperature, top-k, top-p sampling to produce diverse outputs.</p>
</details>
</div>

<div class="section">
<h2>5. Preference Data</h2>
<pre><code>prefs = [
    {"prompt": "What is quantum computing?",
     "chosen": "Quantum computing uses qubits and superposition.",
     "rejected": "Quantum computing is magic."}
]</code></pre>
<div class="tip">ðŸ’¡ Tip: Curate diverse preference pairs for broad LLM alignment.</div>
</div>

<div class="section">
<h2>6. Reward Model Training</h2>
<pre><code>import torch.nn as nn

class RewardModel(nn.Module):
    def __init__(self, hidden_size=768):
        super().__init__()
        self.head = nn.Linear(hidden_size, 1)
    def forward(self, hidden_states):
        return self.head(hidden_states.mean(dim=1))</code></pre>
<details>
<summary>ðŸ’¡ Tip</summary>
<p>Train reward model using pairwise loss. Automates scoring to reduce human workload.</p>
</details>
</div>

<div class="section">
<h2>7. RL Optimization (PPO/DPO)</h2>
<pre><code>for batch in generated_batches:
    rewards = reward_model(batch)
    # PPO: compute advantage, apply clipped surrogate objective
    # DPO: maximize likelihood of preferred responses</code></pre>
<div class="tip">ðŸ’¡ Tip: Start with toy datasets. Clip updates and track entropy for stability.</div>
</div>

<div class="section">
<h2>8. Evaluation</h2>
<pre><code>pre_train_output = policy.generate(**inputs, max_length=50)
post_train_output = policy.generate(**inputs, max_length=50)
print("Before:", tok.decode(pre_train_output[0]))
print("After:", tok.decode(post_train_output[0]))</code></pre>
<div class="tip">ðŸ’¡ Tip: Combine automated metrics with human evaluation.</div>
</div>

<div class="section">
<h2>9. Conclusion & Next Steps</h2>
<table>
<tr><th>Step</th><th>Description</th></tr>
<tr><td>1</td><td>Loaded base LLM</td></tr>
<tr><td>2</td><td>Fine-tuned with supervised data</td></tr>
<tr><td>3</td><td>Generated candidate responses</td></tr>
<tr><td>4</td><td>Created preference pairs</td></tr>
<tr><td>5</td><td>Trained reward model</td></tr>
<tr><td>6</td><td>Optimized policy (PPO/DPO)</td></tr>
<tr><td>7</td><td>Evaluated improvements</td></tr>
</table>
<details>
<summary>ðŸ’¡ Key Insight</summary>
<p>RLHF is iterative, preference-based learning that transforms a generic LLM into a human-aligned AI.</p>
</details>
</div>

</body>
</html>
