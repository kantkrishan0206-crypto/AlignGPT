# AlignGPT
“This project demonstrates a minimal RLHF loop for aligning language models with human preferences. It includes supervised fine-tuning, preference data collection, reward model training, and policy optimization using PPO or DPO. Designed for clarity, reproducibility, and scalability.”
<img width="1600" height="700" alt="image" src="https://github.com/user-attachments/assets/182c9698-28da-412b-8826-dc531c97fb1f" />
