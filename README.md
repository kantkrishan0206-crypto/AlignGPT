# AlignGPT
“This project demonstrates a minimal RLHF loop for aligning language models with human preferences. It includes supervised fine-tuning, preference data collection, reward model training, and policy optimization using PPO or DPO. Designed for clarity, reproducibility, and scalability.”
<img width="1200" height="675" alt="image" src="https://github.com/user-attachments/assets/e33db863-f869-44c0-a0de-9ffd87c4f957" />
