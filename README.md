# AlignGPT
“This project implements a mini LLM alignment pipeline using Reinforcement Learning from Human Feedback (RLHF). It includes training a reward model from human-annotated preference data, fine-tuning the language model via policy optimization, and performing ablation studies to evaluate robustness, fairness, and alignment trade-offs.”
<img width="1200" height="675" alt="image" src="https://github.com/user-attachments/assets/e33db863-f869-44c0-a0de-9ffd87c4f957" />
