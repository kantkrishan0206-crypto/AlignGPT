# AlignGPT
“This project demonstrates a minimal RLHF loop for aligning language models with human preferences. It includes supervised fine-tuning, preference data collection, reward model training, and policy optimization using PPO or DPO. Designed for clarity, reproducibility, and scalability.”
https://miro.medium.com/v2/resize:fit:1400/1*qTV0t3HhwMYU7Hsb2ItzlA.png
