# RLHF-Lab Overview

## Project Overview

RLHF-Lab is a research and experimentation framework for Reinforcement Learning from Human Feedback (RLHF) tailored for Large Language Models (LLMs). This repository provides an end-to-end pipeline to train, evaluate, and compare different fine-tuning methods that incorporate human preferences, including:

* Supervised Fine-Tuning (SFT)
* Reward Modeling (RM)
* Proximal Policy Optimization (PPO)
* Direct Preference Optimization (DPO)

The goal is to understand how LLMs can be aligned with human preferences and values by simulating preference data, running preference-based training, and evaluating the resulting models.

---

## Key Features

1. **SFT Module**

   * Provides supervised fine-tuning of a base LLM on prompt-response pairs.
   * Supports batching, gradient accumulation, mixed-precision, and checkpointing.

2. **Reward Modeling Module**

   * Trains a reward model to score LLM outputs according to preference data.
   * Provides batch-wise scoring, evaluation metrics, and checkpointing.

3. **PPO Training Module**

   * Implements policy-gradient-based RLHF using Proximal Policy Optimization.
   * Integrates reward model scoring and reference model baselines.

4. **DPO Training Module**

   * Implements Direct Preference Optimization, a simpler alternative to PPO.
   * Uses pairwise preference data to optimize the policy directly.

5. **Data Utilities**

   * Scripts to prepare prompts, generate candidate responses, and create preference pairs.
   * Supports JSONL input format for training and evaluation.

6. **Evaluation Suite**

   * Automatic and human-in-the-loop evaluation protocols.
   * Metrics include pairwise accuracy, mean margin, and preference alignment scores.

7. **General Model Utilities**

   * Base model wrapper with save/load, device management, and parameter summary.
   * PEFT/adapter support, quantization helpers (bitsandbytes), and model registry.

8. **Configuration & Scripts**

   * YAML-based configuration for each module (SFT, RM, PPO, DPO).
   * Run scripts for each training module.

9. **Tests**

   * Unit tests for data processing, reward modeling, trainers, and model wrappers.

---

## Directory Structure

```
rlhf-lab/
├─ README.md
├─ LICENSE
├─ docs/                  # Project documentation
├─ configs/               # YAML configs for SFT, RM, PPO, DPO
├─ data/                  # Prompts, SFT data, preference pairs
├─ src/                   # Source code
│  ├─ models/             # Policy, reward, tokenizer, general wrappers
│  ├─ training/           # SFT, RM, PPO, DPO trainers
│  ├─ data/               # Scripts to prepare datasets
│  ├─ eval/               # Evaluation metrics and scripts
│  └─ utils/              # Logging, checkpointing, seeding
├─ scripts/               # Run scripts for training
├─ tests/                 # Unit tests
└─ environment.yml        # Conda environment
```

---

## Workflow Overview

1. **Data Preparation**

   * Build prompts and candidate responses.
   * Generate preference pairs (`chosen` vs `rejected`).
   * Prepare supervised fine-tuning dataset.

2. **Supervised Fine-Tuning (SFT)**

   * Train a base LLM on prompt-response pairs.
   * Save the checkpoint as the initial policy model.

3. **Reward Model (RM) Training**

   * Train a reward model to score outputs using preference pairs.
   * Save RM checkpoint for RLHF training.

4. **RLHF Training (PPO / DPO)**

   * Load policy model from SFT.
   * Use reward model to compute preference rewards.
   * Train using PPO or DPO.
   * Save policy checkpoints after training.

5. **Evaluation**

   * Run automatic evaluation metrics.
   * Optionally perform human-in-the-loop evaluation.

---

## References

* Christiano et al., *Deep Reinforcement Learning from Human Preferences*, NeurIPS 2017.
* Ouyang et al., *Training language models to follow instructions with human feedback*, NeurIPS 2022.
* DPO: S. Raffel et al., *Direct Preference Optimization for Language Models*.
* HuggingFace Transformers: [https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)
* PEFT: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)
* BitsandBytes: [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

