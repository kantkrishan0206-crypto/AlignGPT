# configs/rm_gpt2.yaml
# Reward Model config tuned for a small/medium experiment using a GPT-2 backbone.
# Edit these values to suit your hardware (GPU/CPU) and dataset size.

# ----------------------------------------
# Model & tokenizer
# ----------------------------------------
model_name_or_path: "gpt2"                     # HF model id or local path
tokenizer_name_or_path: null                  # null -> use model_name_or_path
local_files_only: false                       # set true to avoid HF hub downloads

# device: "cuda" or "cpu". Keep 'cuda' for GPU machines.
device: "cuda"

# ----------------------------------------
# Input / data
# ----------------------------------------
train_file: "data/pref_pairs.jsonl"           # JSONL lines: {"prompt": "...", "chosen": "...", "rejected": "..."}
val_file: "data/pref_pairs_val.jsonl"         # optional validation file (can be null)
max_items: null                               # limit items for quick smoke-tests (null = no limit)
max_length: 256                               # max tokens for tokenizer/backbone input
sep_token: " "                                # if using concatenation style, can be changed (the code uses tokenizer.sep_token when available)

# ----------------------------------------
# Model head / pooling
# ----------------------------------------
hidden_pool: "mean"                           # pooling: 'cls' | 'mean' | 'last'
dropout: 0.1
head_hidden: 512                              # hidden size of scalar head
head_activation: "tanh"                       # 'tanh' | 'relu'

# ----------------------------------------
# Training hyperparameters
# ----------------------------------------
train_batch_size: 16                          # per-device batch size (remember gradient_accumulation_steps)
eval_batch_size: 32
gradient_accumulation_steps: 2
max_epochs: 5
max_steps: null                               # optional absolute max steps (null -> use epochs)
save_every: 1000                              # save checkpoint every N optimizer steps
save_total_limit: 5                            # keep last N checkpoints
seed: 42

# ----------------------------------------
# Optimizer & scheduler
# ----------------------------------------
optimizer:
  type: "adamw"
  lr: 2e-5
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  type: "linear"           # linear | cosine | none
  warmup_ratio: 0.03       # portion of total steps as warmup (overrides warmup_steps if both provided)
  warmup_steps: null       # optional explicit warmup step count (null -> use warmup_ratio)

max_grad_norm: 1.0

# ----------------------------------------
# Mixed precision & performance
# ----------------------------------------
use_amp: true             # use torch.cuda.amp (automatically enabled only if device == cuda)
fp16: true                # shorthand for use_amp (kept for backward compatibility)
gradient_checkpointing: false
num_workers: 4            # dataloader num_workers

# ----------------------------------------
# PEFT / LoRA (optional)
# ----------------------------------------
use_peft: false
peft_adapter_path: null
# Example peft_adapter_path: "./adapters/lora_rm_v1"

# ----------------------------------------
# Loss / training specifics & misc
# ----------------------------------------
loss:
  type: "pairwise"        # 'pairwise' (Bradley-Terry logistic) | 'margin' | 'regression'
  margin: 1.0             # used when margin loss selected

evaluation:
  strategy: "steps"       # 'steps' or 'epoch'
  eval_steps: 200
  compute_metrics: true

logging:
  level: "INFO"           # DEBUG | INFO | WARNING
  wandb: false
  wandb_project: "rlhf-rm"
  wandb_run_name: null

# ----------------------------------------
# Output & checkpoints
# ----------------------------------------
out_dir: "./checkpoints/rm_gpt2"
checkpoint_prefix: "rm"
overwrite_output_dir: false

# ----------------------------------------
# Early stopping & validation
# ----------------------------------------
early_stopping:
  enabled: false
  metric: "loss"
  patience: 3
  min_delta: 0.0

# ----------------------------------------
# Debug / smoke test options
# ----------------------------------------
debug:
  fast_dev_run: false    # if true, run a tiny epoch with a few samples for quick test
  fast_dev_limit: 16     # number of samples to use in fast_dev_run
  verbose_batch: false

# ----------------------------------------
# Notes & examples
# ----------------------------------------
# To run locally (from project root):
#   python src/training/rm_trainer.py --config configs/rm_gpt2.yaml
#
# For a quick smoke test (CPU-only), set device: "cpu" and reduce train_batch_size to 2 and
# gradient_accumulation_steps to 1 in this YAML, or run:
#   python src/training/rm_trainer.py --config configs/rm_gpt2.yaml --max_items 32
#
# Make sure `data/pref_pairs.jsonl` exists and contains lines like:
# {"prompt":"Q: What is AI?","chosen":"AI is the study of...","rejected":"AI is a sandwich."}
#
# If you want to use a BERT-like encoder for reward scoring, set model_name_or_path to
# a sentence/encoder model (e.g., "sentence-transformers/paraphrase-MiniLM-L6-v2"),
# but then ensure RewardModel pooling matches (mean/cls).
