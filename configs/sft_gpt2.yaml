# Supervised Fine-Tuning config for GPT-2 (SFT)
# Purpose: a market-ready, practical config for local experiments (VS Code / single GPU)
# Notes:
#  - Tweak batch sizes, fp16, and device/accumulation settings to match your GPU memory.
#  - To use LoRA/PEFT, set use_peft: true and provide a peft_config or peft_adapter_path.
#  - If you use the HF Trainer with `TrainingArguments`, many of these fields map directly.

# -----------------------------------------------------------------------------
# Model / Tokenizer
# -----------------------------------------------------------------------------
model:
  # HF model id or local path
  model_name_or_path: "gpt2"
  # Optional tokenizer override (if you trained a custom tokenizer)
  tokenizer_name_or_path: "gpt2"
  # Whether to force loading only local files (helpful in restricted environments)
  local_files_only: false

# -----------------------------------------------------------------------------
# Data
# -----------------------------------------------------------------------------
data:
  # SFT JSONL with one JSON per line: {"prompt": "...", "response": "..."}
  train_file: "data/sft.jsonl"
  # Optional validation file for eval during training
  eval_file: "data/sft_val.jsonl"
  # Maximum sequence length for tokenizer/truncation
  max_seq_length: 512
  # If you want to limit data for smoke tests during development
  max_train_samples: null   # set to an integer to limit samples (e.g., 2000)

# -----------------------------------------------------------------------------
# Training hyperparameters (maps to HF TrainingArguments)
# -----------------------------------------------------------------------------
training:
  output_dir: "./checkpoints/sft/gpt2"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8   # effective batch = batch_size * accumulation
  learning_rate: 2e-5
  weight_decay: 0.01
  adam_epsilon: 1e-8
  warmup_ratio: 0.03
  max_grad_norm: 1.0
  lr_scheduler_type: "linear"  # choices: linear, cosine, constant
  logging_steps: 50
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3
  fp16: true                     # use mixed precision if you have a compatible GPU
  gradient_checkpointing: false  # turn on to save memory during forward pass (slower)
  dataloader_num_workers: 4
  remove_unused_columns: false
  push_to_hub: false
  hub_model_id: null

# -----------------------------------------------------------------------------
# PEFT / LoRA (optional) - set use_peft true to enable
# -----------------------------------------------------------------------------
peft:
  use_peft: false
  # if you already have a trained adapter you can supply the path
  peft_adapter_path: null
  # Alternatively specify a small LoRA config to create adapters on-the-fly
  # Example values for rapid prototyping; tune for larger models
  lora:
    r: 8
    alpha: 32
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "v_proj"
      - "k_proj"
      - "o_proj"

# -----------------------------------------------------------------------------
# Checkpointing & logging
# -----------------------------------------------------------------------------
logging:
  logging_dir: "./runs/sft_gpt2"
  report_to: ["tensorboard"]   # can also use ["wandb"] if configured
  run_name: "sft_gpt2_experiment"
  # Save tokenizer and model when saving checkpoints
  save_tokenizer: true

# -----------------------------------------------------------------------------
# Evaluation & Generation during training
# -----------------------------------------------------------------------------
generation:
  do_sample: false
  max_new_tokens: 128
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  num_beams: 1

# evaluation metrics to run after generation (these are computed on generated responses)
evaluation:
  run_generation_eval: true
  # small, fast metrics to include; implement metrics in src/eval/metrics.py
  metrics: ["bleu", "rouge_l", "distinct_1", "distinct_2"]
  # number of prompts to sample from eval set to generate during each eval step
  eval_num_samples: 50

# -----------------------------------------------------------------------------
# Environment & reproducibility
# -----------------------------------------------------------------------------
env:
  seed: 42
  # recommended: set this to false while iterating locally, set to true for reproducible experiments
  deterministic: false
  # path to a file to store a list of exact training samples used (useful for reproducibility)
  sample_manifest: "./checkpoints/sft/sample_manifest.jsonl"

# -----------------------------------------------------------------------------
# Autoscaling / cluster hints (optional, not used by HF Trainer directly)
# -----------------------------------------------------------------------------
cluster:
  use_deepspeed: false
  deepspeed_config: null
  use_accelerate: false

# -----------------------------------------------------------------------------
# Debug / dev options
# -----------------------------------------------------------------------------
dev:
  # quick run for testing (overrides epochs / dataset size)
  quick_dev_run: false
  quick_dev_max_samples: 64
  # sanity checks performed at startup
  run_sanity_checks: true

# -----------------------------------------------------------------------------
# Example overrides for fine-grained control
# -----------------------------------------------------------------------------
overrides:
  # If you want to freeze large parts of the model and only train adapters
  freeze_backbone: false
  # pattern of parameter names to unfreeze (substring match)
  unfreeze_pattern: null
  # whether to compute loss only on response tokens by masking the prompt positions
  mask_prompt_tokens: true

# -----------------------------------------------------------------------------
# Notes & tips (non-parsed comments):
# - If running on a single 16GB GPU, try: per_device_train_batch_size: 1, gradient_accumulation_steps: 16, fp16: true.
# - To push to HF hub, set push_to_hub true and provide hub_model_id and credentials in HF CLI.
# - For PEFT/LoRA small experiments prefer r=8, alpha=32; for larger models reduce r to 4 or lower memory.
# - Make sure data/sft.jsonl contains 
#     {"prompt": "<instruction>", "response": "<expected reply>"}
#   and that you have a valid tokenizer compatible with the model.

