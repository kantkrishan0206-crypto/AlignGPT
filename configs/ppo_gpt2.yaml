# configs/ppo_gpt2.yaml
# PPO config for rlhf-lab (default: small-local experiment)
# - Target: run locally on a single GPU (NVIDIA), works in VS Code
# - Change model_name_or_path to a larger model for bigger runs
#
# Usage:
#   python src/training/ppo_trainer.py --config configs/ppo_gpt2.yaml

# ---------------------------
# Model & Tokenizer
# ---------------------------
model:
  # Default model for local runs. Use "gpt2-medium" or a HF repo for larger runs.
  model_name_or_path: "distilgpt2"
  # If you use PEFT/LoRA, set use_peft: true and provide adapter path below.
  use_peft: false
  peft_adapter_path: null
  # HuggingFace trust_remote_code if loading custom checkpoints
  trust_remote_code: false
  # If you want 8-bit loading (requires bitsandbytes), set true and ensure BNB installed.
  load_in_8bit: false
  # Max sequence length for tokenization and generation
  max_seq_length: 512

tokenizer:
  # If you trained a custom tokenizer, put path or hub id here
  tokenizer_name_or_path: null
  # Use fast tokenizer (recommended)
  use_fast: true
  # special tokens (optional)
  pad_token: "<|pad|>"
  bos_token: "<|bos|>"
  eos_token: "<|eos|>"

# ---------------------------
# Reward model (scoring)
# ---------------------------
reward:
  # Path or HF id of the trained reward model (must exist)
  reward_model_path: "checkpoints/rm/final"
  # Tokenizer max length used by reward model (should match reward model config)
  max_length: 256

# ---------------------------
# Data / prompts
# ---------------------------
data:
  prompts_file: "data/prompts.jsonl"   # one JSON-line per prompt: {"prompt": "..."}
  # Optionally provide a small eval prompts file for generation evaluation
  eval_prompts_file: "data/prompts_eval.jsonl"
  # batch size of prompt sampling for rollouts (how many prompts to generate per step)
  batch_size: 4

# ---------------------------
# Generation / Rollouts
# ---------------------------
generation:
  # how many tokens the policy is allowed to append per prompt
  gen_max_new_tokens: 64
  # sampling parameters (set do_sample True in trainer call)
  gen_temperature: 1.0
  gen_top_k: 50
  gen_top_p: 0.95
  # if true, use sampling (recommended for RL exploration)
  do_sample: true
  # stop token(s) or stopping criteria (null uses eos token)
  stopping_tokens: null

# ---------------------------
# PPO / RL hyperparameters
# ---------------------------
ppo:
  # Collecting & update
  total_steps: 2000            # total prompt-samples processed (small smoke test)
  rollout_size: 64             # number of rollouts to collect before each update
  batch_size: 4                # how many prompts to sample each collection loop
  ppo_epochs: 4                # how many epochs per PPO update over the collected rollouts
  minibatch_size: 8            # minibatch size inside PPO update (must divide rollout_size)
  update_epochs: 4

  # PPO loss / coefficients
  clip_epsilon: 0.2
  vf_coef: 0.5                 # value function coefficient
  entropy_coef: 0.01           # small entropy bonus to encourage exploration
  kl_coef: 0.02                # optional KL penalty coefficient to reference policy
  target_kl: null              # if set, scheduler can enforce target KL (not implemented by default)

  # GAE / advantage estimation
  gamma: 0.99
  lam: 0.95

# ---------------------------
# Optimization & LR schedule
# ---------------------------
optim:
  learning_rate: 1.5e-05
  weight_decay: 0.0
  adam_eps: 1e-08
  betas: [0.9, 0.999]
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1

scheduler:
  # linear warmup then linear decay recommended for fine-tuning / RL
  use_scheduler: true
  warmup_steps: 100
  max_train_steps: 2000
  scheduler_type: "linear"   # options: linear, cosine, cosine_with_restarts

# ---------------------------
# Value function
# ---------------------------
value:
  # small MLP head on top of policy hidden states
  hidden_dim: 256
  init_from_policy: true    # bool: reuses policy hidden_size to init value head dims

# ---------------------------
# Checkpointing & logging
# ---------------------------
logging:
  out_dir: "checkpoints/ppo"
  save_every: 500            # save checkpoint every N update steps
  max_checkpoints_keep: 5
  log_with_wandb: false
  wandb_project: "rlhf-ppo"
  wandb_run_name: "ppo_run"

  # console/file logging levels
  level: "INFO"
  logfile: "checkpoints/ppo/training.log"

# ---------------------------
# Evaluation
# ---------------------------
eval:
  eval_every_steps: 500
  eval_num_prompts: 32
  generation_eval:
    # automatic metrics to compute (BLEU/ROUGE/LMScore) - keep minimal locally
    compute_bleu: false
    compute_rouge: false
    compute_reward_mean: true

# ---------------------------
# Reference policy (for KL or comparison)
# ---------------------------
reference:
  # If you want to use a frozen reference policy other than the initial policy, set path here.
  reference_model_path: null

# ---------------------------
# Runtime / device
# ---------------------------
runtime:
  device: "cuda"              # "cuda" or "cpu"
  fp16: true                  # mixed precision (requires CUDA)
  seed: 42
  deterministic: false

# ---------------------------
# Misc / debug
# ---------------------------
misc:
  smoke_test_mode: false      # if true, will run with tiny data for quick debug
  debug_num_rollouts: 8
  max_response_length_debug: 64

# ---------------------------
# Notes for scaling out
# ---------------------------
# For multi-GPU or multi-node training:
# - Use accelerate or torch.distributed to shard model & data
# - Set device_map and load_in_8bit if using bitsandbytes for very large models
# - Increase batch_size, rollout_size, total_steps, and gradient_accumulation_steps accordingly
#
# Example changes to run on a larger model:
#   model.model_name_or_path: "gpt2-medium"
#   runtime.fp16: true
#   optim.learning_rate: 1e-5
#   ppo.total_steps: 50000
#   rollout_size: 1024
#   logging.log_with_wandb: true

